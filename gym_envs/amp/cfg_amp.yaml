seed: 1
record_video: True    # Is considered False if <environment.render> = False below

environment:
  render: True
  num_envs: 512
  eval_every_n: 100
  save_every_n: 100
  num_threads: '60'
  simulation_dt: 0.0025
  control_dt: 0.04
  max_time: 24.0
  log_interval: 8
  enable_dynamics_randomization: False
  episode_length_for_dynamics_learning: 2.0
  enable_expert_dataset_initialisation: True

  action_scaling: 1.0

  use_actuator_network: True

  velocity_command:
    limit_heading_velocity: 1.0
    limit_lateral_velocity: 0.75
    limit_yaw_rate: 1.25

    # Less than this is considered to be a zero velocity command
    limit_velocity_magnitude: 0.1

    # These values are rescaled if their sum is not equal to 1
    probability_zero_command: 0.2
    probability_heading_command: 0.0
    probability_lateral_command: 0.0
    probability_yaw_command: 0.0
    probability_direction_command: 0.0
    probability_constant_command: 0.8   # includes all three velocities

    # Time range in seconds when the next command is sampled
    command_sampling_time_min: 3.0
    command_sampling_time_max: 4.0

  reward:
    base_linear_velocity_tracking:
      coeff: 0
    base_angular_velocity_tracking:
      coeff: 0
    style:
      coeff: 2
    joint_torque:
      coeff: -0.01
    joint_velocity:
      coeff: -0.001
    joint_jerk:
      coeff: -0.0001


  observation_indices:
    rotation: [ 0, 3 ]
    joint_position: [ 3, 15 ]
    angular_velocity: [ 15, 18 ]
    joint_velocity: [ 18, 30 ]
    linear_velocity: [ 30, 33 ]
    foot_position: [ 33, 45]
    command: [45, 48]
    joint_position_error: [48, 60]

    positions: [ 0, 15 ]
    velocities: [ 15, 33 ]

    actor_input: [ 0, 60 ]
    critic_input: [ 0, 60 ]
    discriminator_input: [ 0, 90 ]

    recurrent_input: [0, 48]

    dynamics_encoding_input: [ 0, 33 ]
    dynamics_inference_output: [ 0, 33 ]

    synchronous_estimation: None

  curriculum:
    reward_factor: 1.0
    reward_advance_rate: 1.0

module:
  type: 'dense'

  actor:
    # Choose from ['tanh', 'leaky_relu', 'softsign', 'relu']
    activation: 'relu'
    hidden: [1024, 512]

  critic:
    # Choose from ['tanh', 'leaky_relu', 'softsign', 'relu']
    activation: 'relu'
    hidden: [1024, 512]

  discriminator:
    # Choose from ['tanh', 'leaky_relu', 'softsign', 'relu']
    activation: 'relu'
    hidden: [1024, 512]

  properties:
    dense:
      shuffle_batch: True
      predict_values_during_act: False

      initial_action_std: 1.0
      compute_jacobian: True

      network_weights_gain: 1.0

algorithm:
  update_steps: 16
  gamma_half_life_duration: 3.0 # seconds

  learning_rate:
    initial: 5e-4
    final: 5e-4

    min: 5e-4
    decay_steps: 1

    mode: 'constant'

  actor:
    learning_rate: 5e-4
    weight_decay: 0.0

  critic:
    learning_rate: 5e-4

  discriminator:
    learning_rate: 1e-4
    weight_decay: 0.0005
    logit_reg_weight: 0.0
    mini_batch_size: 256
    buffer_size: 1000000
